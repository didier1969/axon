---
phase: 01-consolidation-and-scale
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/axon/core/storage/kuzu_backend.py
  - src/axon/core/ingestion/pipeline.py
  - src/axon/core/embeddings/embedder.py
  - tests/core/test_kuzu_backend.py
  - tests/core/test_pipeline.py
autonomous: true
---

<objective>
## Goal
Batch Cypher inserts for incremental reindexing, decouple embedding generation from the indexing pipeline via async/background execution, and profile the 3 slowest repos to establish a performance baseline.

## Purpose
Indexing times range from 11s to 562s even without embeddings. The incremental path (`reindex_files`) inserts nodes and relationships one-by-one via `add_nodes`/`add_relationships`, which issues a separate Cypher CREATE per item. Batching these into CSV COPY FROM (already used by `bulk_load`) will dramatically cut storage write time. Decoupling embeddings lets the index be queryable immediately while vectors compute in the background.

## Output
- `add_nodes` and `add_relationships` use CSV COPY FROM for batches above a threshold
- `embed_graph` supports a background/async mode via `concurrent.futures`
- `pipeline.py` stores embeddings asynchronously when batch size warrants it
- Profiling data captured for machineflow, flow_analyzer, BookingSystem (saved to `.paul/phases/01-consolidation-and-scale/profiling-baseline.md`)
</objective>

<context>
## Project Context
@.paul/PROJECT.md
@.paul/ROADMAP.md
@.paul/STATE.md
@.paul/MILESTONE-CONTEXT.md

## Source Files
@src/axon/core/storage/kuzu_backend.py
@src/axon/core/storage/base.py
@src/axon/core/ingestion/pipeline.py
@src/axon/core/embeddings/embedder.py
</context>

<acceptance_criteria>

## AC-1: Batch node inserts use CSV COPY FROM
```gherkin
Given a list of GraphNode objects passed to add_nodes
When the list contains more than 1 node
Then the backend writes a temporary CSV and uses COPY FROM instead of individual CREATE statements
And the fallback to individual inserts still works when CSV COPY FROM fails
```

## AC-2: Batch relationship inserts use CSV COPY FROM
```gherkin
Given a list of GraphRelationship objects passed to add_relationships
When the list contains more than 1 relationship
Then the backend groups by (src_table, dst_table) and uses CSV COPY FROM for each group
And the fallback to individual inserts still works when CSV COPY FROM fails
```

## AC-3: Embeddings can run asynchronously
```gherkin
Given the pipeline completes storage loading
When embeddings are enabled
Then embedding generation and storage runs in a background thread via ThreadPoolExecutor
And the pipeline returns immediately with embeddings=0 (background will update)
And a synchronous fallback exists when async is not desired (e.g., CLI --wait-embeddings)
```

## AC-4: Profiling baseline established
```gherkin
Given the 3 slowest repos (machineflow, flow_analyzer, BookingSystem)
When profiled with phase_timings output
Then a baseline document records per-phase timings for each repo
And the document is saved at .paul/phases/01-consolidation-and-scale/profiling-baseline.md
```

## AC-5: All existing tests pass
```gherkin
Given the changes to kuzu_backend.py, pipeline.py, and embedder.py
When the full test suite is run
Then all 678+ tests pass with no regressions
```

</acceptance_criteria>

<tasks>

<task type="auto">
  <name>Task 1: Batch add_nodes and add_relationships via CSV COPY FROM</name>
  <files>src/axon/core/storage/kuzu_backend.py, tests/core/test_kuzu_backend.py</files>
  <action>
    Refactor `add_nodes` to use the existing `_csv_copy` method for batch inserts:
    - Group nodes by table (same logic as `_bulk_load_nodes_csv`)
    - Call `_csv_copy(table, rows)` for each group
    - Wrap in try/except: on failure, fall back to the current one-by-one `_insert_node` loop
    - The `_csv_copy` method and CSV format already exist and are battle-tested by `bulk_load`

    Refactor `add_relationships` similarly:
    - Group rels by (src_table, dst_table) pair (same logic as `_bulk_load_rels_csv`)
    - Call `_csv_copy(f"CodeRelation_{src}_{dst}", rows)` for each group
    - Wrap in try/except: on failure, fall back to the current one-by-one `_insert_relationship` loop

    Add tests:
    - Test that add_nodes with multiple nodes succeeds (integration test with real KuzuDB)
    - Test that add_relationships with multiple rels succeeds
    - Test fallback behavior (mock _csv_copy to raise, verify individual inserts still work)

    Avoid: changing the `bulk_load` method — it already works correctly with CSV COPY FROM.
    Avoid: changing the StorageBackend protocol in base.py — the interface stays the same.
  </action>
  <verify>
    uv run python3 -m pytest tests/core/test_kuzu_backend.py -v
    uv run python3 -m pytest tests/ -x -q  (full suite, 678+ pass)
  </verify>
  <done>AC-1 and AC-2 satisfied: batch inserts use CSV COPY FROM with fallback</done>
</task>

<task type="auto">
  <name>Task 2: Async embedding generation in pipeline</name>
  <files>src/axon/core/ingestion/pipeline.py, src/axon/core/embeddings/embedder.py, tests/core/test_pipeline.py</files>
  <action>
    Modify `run_pipeline` to run embedding generation in a background thread:
    - Import `concurrent.futures.ThreadPoolExecutor`
    - After `storage.bulk_load(graph)`, submit `embed_graph` + `store_embeddings` to a thread
    - Store the future on the PipelineResult (new field: `embedding_future: Future | None = None`)
    - Pipeline returns immediately; caller can optionally `result.embedding_future.result()` to wait
    - The incremental path in run_pipeline should also use async embeddings

    Add a `wait_embeddings` parameter to `run_pipeline` (default: False):
    - When True: block until embeddings complete (current behavior, for CLI `--wait-embeddings`)
    - When False: return immediately, embeddings run in background

    Update the CLI (`analyze` command) to NOT wait by default but log when embeddings finish.
    The `--no-embeddings` flag already exists — no change needed there.

    Add test:
    - Test that pipeline returns before embeddings complete when wait_embeddings=False
    - Test that result.embedding_future is not None when embeddings enabled

    Avoid: making embed_graph itself async — it's CPU-bound, thread is sufficient.
    Avoid: modifying the embedder.py module beyond adding type hints if needed.
  </action>
  <verify>
    uv run python3 -m pytest tests/core/test_pipeline.py -v
    uv run python3 -m pytest tests/ -x -q  (full suite, 678+ pass)
  </verify>
  <done>AC-3 satisfied: embeddings run asynchronously with optional wait</done>
</task>

<task type="auto">
  <name>Task 3: Profile 3 slowest repos and capture baseline</name>
  <files>.paul/phases/01-consolidation-and-scale/profiling-baseline.md</files>
  <action>
    Run `axon analyze` on the 3 slowest repos with phase_timings output:
    - machineflow (~562s without embeddings)
    - flow_analyzer (~515s)
    - BookingSystem (~474s)

    Use `--no-embeddings` flag since we're measuring core pipeline performance.
    Capture output including per-phase timings (walk, structure, parsing, imports, calls, heritage, types, communities, processes, dead_code, coupling, storage_load).

    Write results to `.paul/phases/01-consolidation-and-scale/profiling-baseline.md`:
    - Table with per-phase timings for each repo
    - Identify the top 3 bottleneck phases across all repos
    - Note total file counts and symbol counts for context
    - Record before/after for the batch insert changes (run profiling AFTER tasks 1-2)

    Avoid: running with embeddings — fastembed adds significant time that's not relevant to this profiling.
  </action>
  <verify>
    File exists: .paul/phases/01-consolidation-and-scale/profiling-baseline.md
    Contains per-phase timing data for all 3 repos
  </verify>
  <done>AC-4 satisfied: profiling baseline document created with per-phase timings</done>
</task>

</tasks>

<boundaries>

## DO NOT CHANGE
- src/axon/core/storage/base.py (StorageBackend protocol — interface stays the same)
- src/axon/core/graph/* (graph model is stable)
- src/axon/mcp/* (MCP tools untouched in this plan)
- src/axon/core/ingestion/community.py (community detection is a separate concern)
- tests/e2e/* (e2e tests run as-is for regression)

## SCOPE LIMITS
- No changes to the `bulk_load` method — it already uses CSV COPY FROM correctly
- No kuzu_backend.py refactoring/splitting — that's plan 01-02
- No error handling cleanup — that's plan 01-02
- No new language parsers or markdown changes
- Profiling is observational only — no optimization of individual phases yet (beyond batch inserts)

</boundaries>

<verification>
Before declaring plan complete:
- [ ] `uv run python3 -m pytest tests/ -x -q` — all 678+ tests pass
- [ ] `add_nodes` with >1 node uses CSV COPY FROM (verify via debug log or test)
- [ ] `add_relationships` with >1 rel uses CSV COPY FROM
- [ ] Pipeline returns before embeddings finish (when wait_embeddings=False)
- [ ] Profiling baseline document exists with data for 3 repos
- [ ] No regressions in incremental reindexing path
</verification>

<success_criteria>
- All tasks completed
- All verification checks pass
- No errors or warnings introduced
- 678+ tests pass (only additions, no removals)
- Profiling baseline captured for future comparison
</success_criteria>

<output>
After completion, create `.paul/phases/01-consolidation-and-scale/01-01-SUMMARY.md`
</output>
