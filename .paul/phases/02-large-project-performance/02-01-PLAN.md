---
phase: 02-large-project-performance
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/axon/core/ingestion/pipeline.py
  - benchmarks/run_benchmark.py
autonomous: true
---

<objective>
## Goal
Add per-phase timing instrumentation to the pipeline and create a benchmark CLI that measures where time is spent indexing a large repository.

## Purpose
Before optimising (Plans 02-02 and 02-03), we need data. This plan instruments `run_pipeline()` so every phase records its own duration, then creates a standalone `benchmarks/run_benchmark.py` script that runs the full pipeline on any local repo and reports a phase-by-phase timing table — identifying the bottleneck.

The success metric from PROJECT.md is: <60s for 100k LOC. This plan establishes the current baseline.

## Output
- `PipelineResult.phase_timings` — `PhaseTimings` dataclass with a float per named phase
- `benchmarks/run_benchmark.py` — runnable benchmark CLI
</objective>

<context>
## Project Context
@.paul/PROJECT.md
@.paul/ROADMAP.md
@.paul/STATE.md

## Source Files
@src/axon/core/ingestion/pipeline.py
@tests/core/test_pipeline.py
</context>

<acceptance_criteria>

## AC-1: Per-phase timing captured in PipelineResult
```gherkin
Given the pipeline runs on any repository
When run_pipeline() completes
Then result.phase_timings is a PhaseTimings dataclass with named float fields
And each field holds the wall-clock seconds for that phase (>= 0.0)
And the sum of all phase timing fields approximates result.duration_seconds (within 10%)
```

## AC-2: Benchmark CLI produces a timing report
```gherkin
Given a local repository path
When python benchmarks/run_benchmark.py --repo-path <path> runs
Then stdout shows a summary table: files, symbols, and per-phase durations in descending order
And it reports the slowest phase explicitly (e.g. "Bottleneck: Parsing code (12.3s)")
And exit code is 0
```

## AC-3: Existing tests unaffected
```gherkin
Given PipelineResult is extended with a new phase_timings field
When the full test suite runs (uv run pytest)
Then all existing tests pass (≥ 645)
And no existing assertions on PipelineResult break
```

</acceptance_criteria>

<tasks>

<task type="auto">
  <name>Task 1: Instrument pipeline phases with per-phase timing</name>
  <files>src/axon/core/ingestion/pipeline.py</files>
  <action>
    Add a `PhaseTimings` dataclass immediately before `PipelineResult`:

    ```python
    @dataclass
    class PhaseTimings:
        """Wall-clock duration (seconds) for each pipeline phase."""
        walk: float = 0.0
        structure: float = 0.0
        parsing: float = 0.0
        imports: float = 0.0
        calls: float = 0.0
        heritage: float = 0.0
        types: float = 0.0
        communities: float = 0.0
        processes: float = 0.0
        dead_code: float = 0.0
        coupling: float = 0.0
        storage_load: float = 0.0
        embeddings: float = 0.0
    ```

    Add `phase_timings: PhaseTimings = field(default_factory=PhaseTimings)` to `PipelineResult`.
    Import `field` from `dataclasses` at the top.

    In `run_pipeline()`, wrap each phase call with a `_t = time.monotonic()` / `result.phase_timings.X = time.monotonic() - _t` pair. Example:

    ```python
    _t = time.monotonic()
    files = walk_repo(repo_path, gitignore)
    result.phase_timings.walk = time.monotonic() - _t
    ```

    Apply this pattern to all 13 named phases (walk, structure, parsing, imports, calls, heritage, types, communities, processes, dead_code, coupling, storage_load, embeddings).

    Do NOT change any public signatures, return types, or existing fields on PipelineResult.
    Do NOT add phase timing to `reindex_files()` or `build_graph()` — those are not the hot path we benchmark.
  </action>
  <verify>
    Run: uv run python -c "
    from pathlib import Path
    from axon.core.ingestion.pipeline import run_pipeline
    graph, result = run_pipeline(Path('.'), embeddings=False)
    pt = result.phase_timings
    total_phases = sum([pt.walk, pt.structure, pt.parsing, pt.imports, pt.calls, pt.heritage, pt.types, pt.communities, pt.processes, pt.dead_code, pt.coupling])
    print(f'walk={pt.walk:.3f}s parsing={pt.parsing:.3f}s total_phases={total_phases:.3f}s pipeline={result.duration_seconds:.3f}s')
    assert pt.walk >= 0 and pt.parsing >= 0
    print('PASS')
    "
    Expected: non-zero values printed with PASS.
  </verify>
  <done>AC-1 satisfied: phase_timings populated on every pipeline run</done>
</task>

<task type="auto">
  <name>Task 2: Create benchmarks/run_benchmark.py CLI</name>
  <files>benchmarks/run_benchmark.py</files>
  <action>
    Create `benchmarks/run_benchmark.py` as a standalone script (no package install required beyond axon itself).

    CLI interface:
    ```
    python benchmarks/run_benchmark.py --repo-path /path/to/repo [--no-embeddings] [--json]
    ```

    Arguments:
    - `--repo-path` (required): Path to the repository to index
    - `--no-embeddings`: Skip embedding generation (default: skip for benchmarking speed)
    - `--json`: Output results as JSON to stdout instead of table

    Behaviour:
    1. Import `run_pipeline` from `axon.core.ingestion.pipeline`
    2. Load gitignore via `axon.config.ignore.load_gitignore`
    3. Run `run_pipeline(repo_path, embeddings=not args.no_embeddings)`
    4. Print a summary block:

    ```
    ═══════════════════════════════════════════════════
    AXON BENCHMARK REPORT
    Repo:     /path/to/repo
    Files:    1234
    Symbols:  5678
    Total:    45.2s
    ═══════════════════════════════════════════════════
    Phase               Duration    % of total
    ───────────────────────────────────────────────────
    Parsing code         12.3s       27.2%
    Analyzing git hist    9.1s       20.1%
    ...
    ═══════════════════════════════════════════════════
    Bottleneck: Parsing code (12.3s, 27.2% of total)
    ═══════════════════════════════════════════════════
    ```

    - Rows sorted by duration descending
    - Phase names match the `PhaseTimings` field names (humanised: "walk" → "File walking", etc.)
    - With `--json`, output a JSON object with all fields instead

    Humanised phase name mapping:
    ```python
    PHASE_NAMES = {
        "walk":         "File walking",
        "structure":    "Structure processing",
        "parsing":      "Parsing code",
        "imports":      "Resolving imports",
        "calls":        "Tracing calls",
        "heritage":     "Extracting heritage",
        "types":        "Analyzing types",
        "communities":  "Detecting communities",
        "processes":    "Detecting execution flows",
        "dead_code":    "Finding dead code",
        "coupling":     "Analyzing git history",
        "storage_load": "Loading to storage",
        "embeddings":   "Generating embeddings",
    }
    ```

    Use only stdlib (argparse, json, dataclasses, sys, pathlib). No new dependencies.

    Add a `if __name__ == "__main__": main()` guard at the bottom.
  </action>
  <verify>
    Run: uv run python benchmarks/run_benchmark.py --repo-path . --no-embeddings
    Expected: benchmark report printed to stdout with non-zero timings, exit code 0.

    Run: uv run python benchmarks/run_benchmark.py --repo-path . --no-embeddings --json | python -c "import sys,json; d=json.load(sys.stdin); print(d['bottleneck_phase'])"
    Expected: prints the name of the bottleneck phase without error.
  </verify>
  <done>AC-2 satisfied: benchmark CLI produces phase timing report</done>
</task>

</tasks>

<boundaries>

## DO NOT CHANGE
- src/axon/core/parsers/* (parsers are stable, Phase 1 work)
- src/axon/core/storage/* (storage layer not in scope)
- src/axon/core/ingestion/watcher.py (watcher optimisations are Phase 2 later)
- tests/* (do not modify any existing tests)
- pyproject.toml (no new dependencies)

## SCOPE LIMITS
- No performance optimisations in this plan — measure only, do not change algorithmic behaviour
- Do not add per-phase timing to reindex_files() or build_graph() — only run_pipeline() is the benchmark target
- No CLI integration (axon CLI command) — benchmarks/ is a standalone developer script
- Embedding generation is skipped by default in the benchmark (--no-embeddings flag) to isolate indexing perf from fastembed model loading

</boundaries>

<verification>
Before declaring plan complete:
- [ ] uv run pytest passes with ≥ 645 tests
- [ ] python -c "from axon.core.ingestion.pipeline import PipelineResult, PhaseTimings; r = PipelineResult(); assert hasattr(r, 'phase_timings'); print('OK')"
- [ ] uv run python benchmarks/run_benchmark.py --repo-path . --no-embeddings exits 0 and prints a timing table
- [ ] uv run python benchmarks/run_benchmark.py --repo-path . --no-embeddings --json outputs valid JSON
- [ ] No regressions: result.duration_seconds and existing PipelineResult fields (files, symbols, etc.) still populated correctly
</verification>

<success_criteria>
- PipelineResult.phase_timings populated on every run_pipeline() call
- benchmarks/run_benchmark.py works against any local repo
- All 645+ existing tests pass
- No new runtime dependencies introduced
</success_criteria>

<output>
After completion, create `.paul/phases/02-large-project-performance/02-01-SUMMARY.md` with:
- What was built
- Sample benchmark output (from running against the axon repo)
- Identified bottleneck(s) to guide Plans 02-02 and 02-03
</output>
