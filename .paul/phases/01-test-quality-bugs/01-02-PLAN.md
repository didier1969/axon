---
phase: 01-test-quality-bugs
plan: 02
type: execute
wave: 1
depends_on: ["01-01"]
files_modified:
  - tests/core/conftest.py
  - tests/core/test_pipeline.py
  - tests/core/test_watcher.py
autonomous: true
---

<objective>
## Goal
Eliminate KuzuDB initialization overhead from per-test fixtures and fix the
flaky async embeddings race condition. Two targeted changes: (1) session-scoped
empty-schema KuzuDB template (copied per test instead of re-created); (2)
session-scoped pre-indexed watcher storage (eliminating run_pipeline() from 8
test bodies).

## Purpose
Plan 01-01 proved the bottleneck is KuzuDB initialization (4-5s per test for
schema creation) and per-test run_pipeline() calls (5-7s each for watcher tests).
This plan eliminates both by amortizing across the session. Watcher tests will
drop from ~102s to <15s. Pipeline tests will improve from ~166s to ~90-100s
(the remaining cost is unavoidable — those tests are testing run_pipeline()).

## Output
- `tests/core/conftest.py` (modified): two session fixtures added
- `tests/core/test_pipeline.py` (modified): storage/rich_storage use template;
  flaky async test fixed
- `tests/core/test_watcher.py` (modified): storage uses pre-indexed template;
  run_pipeline() setup calls removed from 8 tests
</objective>

<context>
## Project Context
@.paul/PROJECT.md
@.paul/STATE.md

## Prior Work
@.paul/phases/01-test-quality-bugs/01-01-SUMMARY.md

## Source Files
@tests/core/conftest.py
@tests/core/test_pipeline.py
@tests/core/test_watcher.py
@src/axon/core/storage/kuzu_backend.py
@src/axon/core/storage/kuzu_schema.py
</context>

<acceptance_criteria>

## AC-1: Flaky async embeddings test no longer races
```gherkin
Given test_async_embeddings_returns_future patches embed_graph in a with block
When the patch context exits before the background thread calls result()
Then the test does not accidentally load the real fastembed model
And the test passes consistently (no timeout, no real model load)
```

## AC-2: test_pipeline.py runs in < 100s
```gherkin
Given 18 tests in tests/core/test_pipeline.py
When running `uv run pytest tests/core/test_pipeline.py -q`
Then the suite completes in under 100 seconds (previously ~166s)
And all 18 tests pass
```
Note: <20s (Plan 01-01 target) was based on a wrong hypothesis. <100s is the
correct target given the real bottleneck is KuzuDB overhead, not embeddings.
Pipeline tests inherently call run_pipeline() — that cost cannot be eliminated.

## AC-3: test_watcher.py runs in < 15s
```gherkin
Given 12 tests in tests/core/test_watcher.py
When running `uv run pytest tests/core/test_watcher.py -q`
Then the suite completes in under 15 seconds (previously ~102s)
And all 12 tests pass
```

## AC-4: Full test suite passes (no regression)
```gherkin
Given all tests in tests/ (excluding e2e)
When running `uv run pytest tests/ --ignore=tests/e2e -q`
Then 0 failures (no regression from fixture changes)
```

</acceptance_criteria>

<tasks>

<task type="auto">
  <name>Task 1: Fix flaky async embeddings race in test_pipeline.py</name>
  <files>tests/core/test_pipeline.py</files>
  <action>
    Fix `test_async_embeddings_returns_future` (line 391-402).

    **Root cause:** The `with patch(...)` context exits BEFORE calling
    `result.embedding_future.result(timeout=10)`. The background thread running
    `embed_graph` is still active when the patch tears down, so it may call the
    REAL `embed_graph` (loading fastembed). This is a race condition.

    **Fix:** Move `result.embedding_future.result(timeout=10)` INSIDE the
    `with patch()` block so the mock is still active when the future resolves.

    Change FROM:
    ```python
    with patch("axon.core.ingestion.pipeline.embed_graph", return_value=[]):
        _, result = run_pipeline(rich_repo, rich_storage, wait_embeddings=False)

    assert result.embedding_future is not None
    # Wait for the background thread to complete.
    result.embedding_future.result(timeout=10)
    ```

    Change TO:
    ```python
    with patch("axon.core.ingestion.pipeline.embed_graph", return_value=[]):
        _, result = run_pipeline(rich_repo, rich_storage, wait_embeddings=False)
        assert result.embedding_future is not None
        # Wait for the background thread while patch is still active.
        result.embedding_future.result(timeout=10)
    ```

    Do NOT change any other test in TestRunPipelineEmbeddings.
  </action>
  <verify>
    Run the specific test 3 times to confirm no race:
    ```
    uv run pytest tests/core/test_pipeline.py::TestRunPipelineEmbeddings::test_async_embeddings_returns_future -v
    uv run pytest tests/core/test_pipeline.py::TestRunPipelineEmbeddings::test_async_embeddings_returns_future -v
    uv run pytest tests/core/test_pipeline.py::TestRunPipelineEmbeddings::test_async_embeddings_returns_future -v
    ```
    All three runs must pass without loading real fastembed.
  </verify>
  <done>AC-1 satisfied: async embeddings test passes consistently without race</done>
</task>

<task type="auto">
  <name>Task 2: Add kuzu_template session fixture; update pipeline storage fixtures</name>
  <files>tests/core/conftest.py, tests/core/test_pipeline.py</files>
  <action>
    ## Part A: Add kuzu_template to conftest.py

    Add a session-scoped fixture that creates ONE empty KuzuDB (schema only, no
    data) per test session. Per-test `storage` fixtures copy this instead of
    re-creating from scratch — saving 4-5s per test.

    Add to `tests/core/conftest.py` (after the existing `isolated_axon_home` fixture):

    ```python
    import shutil

    from axon.core.storage.kuzu_backend import KuzuBackend


    @pytest.fixture(scope="session")
    def kuzu_template(tmp_path_factory: pytest.TempPathFactory) -> Path:
        """Create a schema-initialized empty KuzuDB once per session.

        Function-scoped storage fixtures copy this template instead of
        calling KuzuBackend.initialize() from scratch, saving 4-5s per test.
        The schema is already present in the copy so create_schema() IF NOT
        EXISTS calls become no-ops.
        """
        db_path = tmp_path_factory.mktemp("kuzu_template") / "db"
        backend = KuzuBackend()
        backend.initialize(db_path)
        backend.close()
        return db_path
    ```

    Note: This fixture returns (not yields) because the template is read-only
    after creation — no cleanup needed.

    ## Part B: Update storage and rich_storage in test_pipeline.py

    Replace the existing `storage` fixture (lines 58-65) with:

    ```python
    @pytest.fixture()
    def storage(tmp_path: Path, kuzu_template: Path) -> KuzuBackend:
        """Provide an initialized KuzuBackend by copying the session schema template.

        Saves 4-5s per test vs. calling initialize() from scratch.
        """
        db_path = tmp_path / "test_db"
        shutil.copytree(str(kuzu_template), str(db_path))
        backend = KuzuBackend()
        backend.initialize(db_path)  # all IF NOT EXISTS: no-ops
        yield backend
        backend.close()
    ```

    Replace the existing `rich_storage` fixture (lines 245-252) with:

    ```python
    @pytest.fixture()
    def rich_storage(tmp_path: Path, kuzu_template: Path) -> KuzuBackend:
        """Provide an initialized KuzuBackend for the rich repo tests."""
        db_path = tmp_path / "rich_db"
        shutil.copytree(str(kuzu_template), str(db_path))
        backend = KuzuBackend()
        backend.initialize(db_path)
        yield backend
        backend.close()
    ```

    Add `import shutil` to test_pipeline.py imports (after `from pathlib import Path`).

    Do NOT change any test methods. Only the two fixture definitions change.
  </action>
  <verify>
    ```
    uv run pytest tests/core/test_pipeline.py -q --tb=short
    ```
    All 18 tests must pass. Measure elapsed time (should be <100s).
    If still slow, verify template copy is working by adding a quick timing check:
    ```
    uv run pytest tests/core/test_pipeline.py -q --tb=short 2>&1 | tail -5
    ```
  </verify>
  <done>AC-2 satisfied: test_pipeline.py completes in < 100s, all 18 tests pass</done>
</task>

<task type="auto">
  <name>Task 3: Add watcher_indexed_template; update watcher storage and remove setup calls</name>
  <files>tests/core/conftest.py, tests/core/test_watcher.py</files>
  <action>
    ## Part A: Add watcher_indexed_template to conftest.py

    Add a session-scoped fixture that creates a KuzuDB pre-indexed with the
    watcher test repo structure ONCE per session. Per-test watcher `storage`
    fixtures copy this template — eliminating both the 4-5s KuzuDB init AND
    the 5-7s run_pipeline() call per test.

    The watcher repo structure indexed by this template is identical to what
    `test_watcher.py`'s `tmp_repo` fixture creates:
    - `src/app.py`: `def hello():\n    return 'hello'\n`
    - `src/utils.py`: `def helper():\n    pass\n`

    Path.home() is patched via unittest.mock.patch during indexing (monkeypatch
    is function-scoped and unavailable to session fixtures).

    Add to `tests/core/conftest.py` (after `kuzu_template`):

    ```python
    @pytest.fixture(scope="session")
    def watcher_indexed_template(
        tmp_path_factory: pytest.TempPathFactory,
        kuzu_template: Path,
    ) -> Path:
        """KuzuDB pre-indexed with the standard watcher test repo (src/app.py + src/utils.py).

        Amortizes run_pipeline() cost across all TestReindexFiles and
        TestWatcherReindexFiles tests. Each test copies this template instead of
        calling run_pipeline() from scratch, saving 5-7s per test.

        Path.home() is patched during indexing to prevent events.jsonl pollution.
        """
        from unittest.mock import patch

        from axon.core.ingestion.pipeline import run_pipeline

        # Create repo matching test_watcher.py's tmp_repo fixture exactly.
        repo_dir = tmp_path_factory.mktemp("watcher_template_repo")
        src = repo_dir / "src"
        src.mkdir()
        (src / "app.py").write_text(
            "def hello():\n    return 'hello'\n", encoding="utf-8"
        )
        (src / "utils.py").write_text(
            "def helper():\n    pass\n", encoding="utf-8"
        )

        # Copy empty schema template, index, close.
        db_path = tmp_path_factory.mktemp("watcher_indexed_db") / "db"
        shutil.copytree(str(kuzu_template), str(db_path))

        fake_home = tmp_path_factory.mktemp("watcher_template_home")
        with patch.object(Path, "home", return_value=fake_home):
            backend = KuzuBackend()
            backend.initialize(db_path)
            run_pipeline(repo_dir, backend, embeddings=False)
            backend.close()

        return db_path
    ```

    ## Part B: Update storage fixture in test_watcher.py

    Replace the existing `storage` fixture (lines 41-48) with:

    ```python
    @pytest.fixture()
    def storage(tmp_path: Path, watcher_indexed_template: Path) -> KuzuBackend:
        """Provide a KuzuBackend pre-indexed with src/app.py + src/utils.py.

        Copies the session-level pre-indexed template instead of calling
        run_pipeline() from scratch, saving 9-12s per test.
        """
        db_path = tmp_path / "test_db"
        shutil.copytree(str(watcher_indexed_template), str(db_path))
        backend = KuzuBackend()
        backend.initialize(db_path)  # schema already present: IF NOT EXISTS no-ops
        yield backend
        backend.close()
    ```

    Add `import shutil` to test_watcher.py imports.

    ## Part C: Remove run_pipeline() setup call from 8 test methods

    The storage fixture now provides an already-indexed DB. Remove the first
    `run_pipeline(tmp_repo, storage, embeddings=False)` line from each of these
    8 tests:

    **TestReindexFiles:**
    - `test_reindex_updates_content` (line ~101): remove setup run_pipeline call
    - `test_reindex_handles_new_symbols` (line ~131): remove setup run_pipeline call
    - `test_reindex_removes_deleted_symbols` (line ~155): remove setup run_pipeline call

    **TestWatcherReindexFiles:**
    - `test_reindexes_changed_files` (line ~190): remove setup run_pipeline call
    - `test_skips_ignored_files` (line ~208): remove setup run_pipeline call
    - `test_skips_unsupported_files` (line ~221): remove setup run_pipeline call
    - `test_handles_deleted_files` (line ~233): remove setup run_pipeline call
    - `test_handles_multiple_files` (line ~249): remove setup run_pipeline call

    IMPORTANT: Only remove the `run_pipeline()` line that was used for setup
    (populating initial state). Do NOT remove any other logic or assertions.

    **TestReadFileEntry** does NOT use the `storage` fixture and is unaffected.

    **Why this is safe:** The pre-indexed template DB was created by running the
    exact same `run_pipeline(repo, storage, embeddings=False)` on the same repo
    structure that `tmp_repo` creates. The DB paths are stored as relative paths
    (e.g., `src/app.py`), so the DB is reusable across any `tmp_path`. Each test
    gets an isolated writable copy of the pre-indexed state.
  </action>
  <verify>
    ```
    uv run pytest tests/core/test_watcher.py -q --tb=short
    ```
    All 12 tests must pass in under 15 seconds.
  </verify>
  <done>AC-3 satisfied: test_watcher.py completes in < 15s, all 12 tests pass</done>
</task>

</tasks>

<boundaries>

## DO NOT CHANGE
- `src/axon/core/` — no production code changes (test-only changes)
- `tests/core/test_analytics.py` — already properly isolated
- `TestReadFileEntry` in test_watcher.py — does not use storage, unaffected
- `TestRunPipelineEmbeddings` (other tests) — only fix test_async_embeddings_returns_future
- `TestIncrementalPipeline` — these tests intentionally run pipeline twice and cannot use a pre-indexed template

## SCOPE LIMITS
- Do not convert TestIncrementalPipeline to use session fixtures (they test incremental
  behavior between pipeline runs — they must remain function-scoped and call run_pipeline twice)
- Do not mock KuzuDB (that is Option C, a separate decision)
- Do not change run_pipeline() or KuzuBackend production code
- Do not add new dependencies
- The <20s target for test_pipeline.py is NOT achievable with integration tests;
  the revised target is <100s

</boundaries>

<verification>
Before declaring plan complete:
- [ ] `uv run pytest tests/core/test_pipeline.py::TestRunPipelineEmbeddings::test_async_embeddings_returns_future -v` passes 3 times in a row
- [ ] `uv run pytest tests/core/test_pipeline.py -q` completes in <100s, 18 passed
- [ ] `uv run pytest tests/core/test_watcher.py -q` completes in <15s, 12 passed
- [ ] `uv run pytest tests/core/test_analytics.py -q` passes (6 tests, no regression)
- [ ] `uv run pytest tests/ --ignore=tests/e2e -q` passes overall
- [ ] `wc -l ~/.axon/events.jsonl` unchanged after full test run (AC-1 from Plan 01-01 preserved)
</verification>

<success_criteria>
- Flaky async embeddings test passes consistently (no race with monkeypatch teardown)
- test_pipeline.py: ~166s → < 100s (KuzuDB schema creation amortized via session template)
- test_watcher.py: ~102s → < 15s (both fixture AND run_pipeline() amortized via pre-indexed template)
- Zero test regressions
- events.jsonl isolation from Plan 01-01 preserved
</success_criteria>

<output>
After completion, create `.paul/phases/01-test-quality-bugs/01-02-SUMMARY.md`
</output>
