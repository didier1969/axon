---
phase: 01-test-quality-bugs
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - tests/core/conftest.py
  - tests/core/test_pipeline.py
  - tests/core/test_watcher.py
autonomous: true
---

<objective>
## Goal
Fix three test quality bugs in one shot: analytics pollution, flaky async embeddings,
and a 90s+ watcher test suite.

## Purpose
Tests must be fast, isolated, and reliable so CI is trustworthy and the dev loop is
tight. Currently:
- `~/.axon/events.jsonl` grows with test-named repos after every test run (pollution)
- `test_pipeline.py` runs in 163s (18 tests @ ~9s each) due to real fastembed model
  spawned in background threads from every `run_pipeline()` call without `embeddings=False`
- `test_watcher.py` runs in 90s (12 tests) for the same reason

Root cause for all three: every `run_pipeline()` call without `embeddings=False` spawns
a background `ThreadPoolExecutor` task that loads the fastembed model and writes to
`~/.axon/events.jsonl` via `log_event()`.

## Output
- `tests/core/conftest.py` (new): autouse fixture patching `Path.home()`
- `tests/core/test_pipeline.py` (modified): `embeddings=False` on non-embedding tests
- `tests/core/test_watcher.py` (modified): `embeddings=False` on all setup calls
</objective>

<context>
## Project Context
@.paul/PROJECT.md
@.paul/STATE.md

## Key Source Files
@tests/core/test_pipeline.py
@tests/core/test_watcher.py
@tests/core/test_analytics.py
@src/axon/core/ingestion/pipeline.py

## Root Cause Analysis

`run_pipeline()` signature (pipeline.py:130):
```python
def run_pipeline(
    repo_path, storage=None, *,
    embeddings=True,        # ← default True: spawns background fastembed thread
    wait_embeddings=False,
    ...
)
```

When `embeddings=True` (default), the pipeline:
1. Calls `_EMBEDDING_POOL.submit(_run_embeddings, ...)` — spawns a ThreadPoolExecutor background task
2. This task loads the fastembed model (~4-8s first load, then faster)
3. After embedding, calls `log_event("index", repo=repo_path.name)` — writes to `~/.axon/events.jsonl`

Affected tests (all call `run_pipeline()` without `embeddings=False`):
- `TestRunPipelineBasic.test_run_pipeline_basic` (test_pipeline.py)
- `TestRunPipelineFileCount.test_run_pipeline_file_count`
- `TestRunPipelineFindsSymbols.test_run_pipeline_finds_symbols`
- `TestRunPipelineFindsRelationships.test_run_pipeline_finds_relationships`
- `TestRunPipelineProgressCallback.test_run_pipeline_progress_callback`
- `TestRunPipelineLoadsToStorage.test_run_pipeline_loads_to_storage`
- `TestRunPipelineFullPhases.test_run_pipeline_full_phases`
- `TestRunPipelineProgressIncludesNewPhases.test_run_pipeline_progress_includes_new_phases`
- All `TestIncrementalPipeline` tests (6x `run_pipeline()` calls)
- All `TestReindexFiles` tests in test_watcher.py (setup call each)
- All `TestWatcherReindexFiles` tests in test_watcher.py (setup call each)

`test_analytics.py` is already properly isolated (has `monkeypatch.setattr(Path, "home", ...)` per test).
</context>

<acceptance_criteria>

## AC-1: No events.jsonl pollution from test runs
```gherkin
Given ~/.axon/events.jsonl exists with N entries before the test run
When running the full test suite with `uv run pytest tests/ -q`
Then ~/.axon/events.jsonl still has N entries (no new entries added)
```

## AC-2: test_pipeline.py runs in <20s
```gherkin
Given 18 tests in tests/core/test_pipeline.py
When running `uv run pytest tests/core/test_pipeline.py -q`
Then the suite completes in under 20 seconds (previously 163s)
And all 18 tests pass
```

## AC-3: test_watcher.py runs in <15s
```gherkin
Given 12 tests in tests/core/test_watcher.py
When running `uv run pytest tests/core/test_watcher.py -q`
Then the suite completes in under 15 seconds (previously 90s)
And all 12 tests pass
```

## AC-4: test_analytics.py still passes without changes
```gherkin
Given test_analytics.py has its own explicit monkeypatch per test
When running `uv run pytest tests/core/test_analytics.py -q`
Then all 6 tests pass (no regression from autouse fixture)
```

## AC-5: Full test suite passes
```gherkin
Given all tests in tests/
When running `uv run pytest tests/ -q --ignore=tests/e2e`
Then 0 failures (baseline must be preserved)
```

</acceptance_criteria>

<tasks>

<task type="auto">
  <name>Task 1: Create tests/core/conftest.py with autouse axon home isolation</name>
  <files>tests/core/conftest.py</files>
  <action>
    Create `tests/core/conftest.py` with a single autouse fixture that patches `Path.home()`
    to `tmp_path` for every test in `tests/core/`:

    ```python
    """Shared test fixtures for tests/core/."""
    from __future__ import annotations

    from pathlib import Path

    import pytest


    @pytest.fixture(autouse=True)
    def isolated_axon_home(tmp_path: Path, monkeypatch: pytest.MonkeyPatch) -> None:
        """Redirect Path.home() to tmp_path for every test.

        Prevents log_event() calls inside run_pipeline() from writing to the
        real ~/.axon/events.jsonl during test runs.
        """
        monkeypatch.setattr(Path, "home", lambda: tmp_path)
    ```

    This covers all tests in the package automatically. The existing explicit
    `monkeypatch.setattr(Path, "home", ...)` calls in `test_analytics.py` become
    redundant but harmless (double-patching is fine — monkeypatch is idempotent).

    Do NOT touch test_analytics.py — its explicit patches are fine to keep.
  </action>
  <verify>
    Record line count in ~/.axon/events.jsonl before and after:
    ```
    wc -l ~/.axon/events.jsonl
    uv run pytest tests/core/test_analytics.py tests/core/test_pipeline.py --no-header -q
    wc -l ~/.axon/events.jsonl
    ```
    Line count must not change. All tests must pass.
  </verify>
  <done>AC-1 satisfied: test runs no longer pollute ~/.axon/events.jsonl</done>
</task>

<task type="auto">
  <name>Task 2: Add embeddings=False to non-embedding test_pipeline.py calls</name>
  <files>tests/core/test_pipeline.py</files>
  <action>
    For every `run_pipeline()` call OUTSIDE the `TestRunPipelineEmbeddings` class,
    add `embeddings=False`. This prevents the background fastembed thread from spawning,
    which is the cause of both slowness (~9s/test) and potential race conditions with
    fixture teardown.

    Specific locations (each class, each test):

    **TestRunPipelineBasic** (line ~79):
    `run_pipeline(tmp_repo, storage)` → `run_pipeline(tmp_repo, storage, embeddings=False)`

    **TestRunPipelineFileCount** (line ~95):
    Same change.

    **TestRunPipelineFindsSymbols** (line ~111):
    Same change.

    **TestRunPipelineFindsRelationships** (line ~127):
    Same change.

    **TestRunPipelineProgressCallback** (line ~149):
    `run_pipeline(tmp_repo, storage, progress_callback=callback)` →
    `run_pipeline(tmp_repo, storage, progress_callback=callback, embeddings=False)`

    **TestRunPipelineLoadsToStorage** (line ~175):
    Same as Basic.

    **TestRunPipelineFullPhases** (line ~266):
    `run_pipeline(rich_repo, rich_storage)` → `run_pipeline(rich_repo, rich_storage, embeddings=False)`

    **TestRunPipelineProgressIncludesNewPhases** (line ~310):
    `run_pipeline(rich_repo, rich_storage, progress_callback=callback)` →
    `run_pipeline(rich_repo, rich_storage, progress_callback=callback, embeddings=False)`

    **TestIncrementalPipeline** (6 tests, all `run_pipeline()` calls):
    All already use `embeddings=False` explicitly — verify this is already the case.
    If not, add it.

    **DO NOT touch TestRunPipelineEmbeddings** — those tests specifically test embedding
    behavior and already mock `embed_graph` where appropriate.

    Note on `test_embedding_phase_in_progress`: this test calls
    `run_pipeline(rich_repo, rich_storage, progress_callback=callback, wait_embeddings=True)`
    WITHOUT mocking `embed_graph`. This runs the real fastembed model. Leave it as-is —
    it is intentionally testing the real model and the `wait_embeddings=True` flag means
    it blocks correctly and doesn't race with teardown.
  </action>
  <verify>
    ```
    uv run pytest tests/core/test_pipeline.py -q --tb=short
    ```
    Must complete in <20s with all 18 tests passing.
  </verify>
  <done>AC-2 satisfied: test_pipeline.py runs in under 20s with no failures</done>
</task>

<task type="auto">
  <name>Task 3: Add embeddings=False to all setup run_pipeline() calls in test_watcher.py</name>
  <files>tests/core/test_watcher.py</files>
  <action>
    Every test in `TestReindexFiles` and `TestWatcherReindexFiles` begins with a
    `run_pipeline(tmp_repo, storage)` call to set up initial indexed state. None of
    these tests are testing embedding behavior — they test `reindex_files()` and
    `_reindex_files()` watcher helpers.

    Add `embeddings=False` to every `run_pipeline(tmp_repo, storage)` call in this file:

    **TestReindexFiles.test_reindex_updates_content** (line ~101):
    `run_pipeline(tmp_repo, storage)` → `run_pipeline(tmp_repo, storage, embeddings=False)`

    **TestReindexFiles.test_reindex_handles_new_symbols** (line ~131):
    Same change.

    **TestReindexFiles.test_reindex_removes_deleted_symbols** (line ~155):
    Same change.

    **TestWatcherReindexFiles.test_reindexes_changed_files** (line ~191):
    Same change.

    **TestWatcherReindexFiles.test_skips_ignored_files** (line ~208):
    Same change.

    **TestWatcherReindexFiles.test_skips_unsupported_files** (line ~221):
    Same change.

    **TestWatcherReindexFiles.test_handles_deleted_files** (line ~232):
    Same change.

    **TestWatcherReindexFiles.test_handles_multiple_files** (line ~249):
    Same change.

    The `TestReadFileEntry` class does NOT call `run_pipeline()` — skip it.
  </action>
  <verify>
    ```
    uv run pytest tests/core/test_watcher.py -q --tb=short
    ```
    Must complete in <15s with all 12 tests passing.
  </verify>
  <done>AC-3 satisfied: test_watcher.py runs in under 15s with no failures</done>
</task>

</tasks>

<boundaries>

## DO NOT CHANGE
- `src/axon/core/ingestion/pipeline.py` — do not change production code, only tests
- `src/axon/core/analytics.py` — no production changes
- `tests/core/test_analytics.py` — already properly isolated, do not modify
- `TestRunPipelineEmbeddings` class in test_pipeline.py — embedding tests are correct as-is
- `tests/e2e/` — out of scope

## SCOPE LIMITS
- No changes to how `log_event()` works in production
- No changes to `run_pipeline()` signature or defaults
- No new dependencies
- Do not convert function-scoped fixtures to session-scoped (would break isolation)

</boundaries>

<verification>
Before declaring plan complete:
- [ ] `wc -l ~/.axon/events.jsonl` unchanged after `uv run pytest tests/core/ -q`
- [ ] `uv run pytest tests/core/test_pipeline.py -q` completes in <20s, 18 passed
- [ ] `uv run pytest tests/core/test_watcher.py -q` completes in <15s, 12 passed
- [ ] `uv run pytest tests/core/test_analytics.py -q` passes (6 tests)
- [ ] `uv run pytest tests/ --ignore=tests/e2e -q` passes overall
</verification>

<success_criteria>
- All 3 bugs fixed in one apply pass
- No production code changed (test-only changes)
- events.jsonl not polluted by test runs
- test_pipeline.py: 163s → <20s
- test_watcher.py: 90s → <15s
- Zero test regressions
</success_criteria>

<output>
After completion, create `.paul/phases/01-test-quality-bugs/01-01-SUMMARY.md`
</output>
